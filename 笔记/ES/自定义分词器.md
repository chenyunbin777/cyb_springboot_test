# ES 配置-自定义分词器

## Elasticsearch的停用词
- 1、有些词在文本中出现的频率非常高，但是对文本所携带的信息基本不产生影响。所以需要去掉以保证分词的正确性。
- 2、英文：a、an、the、of
- 3、中文：的、了、着、是 、标点符号等
- 4、文本经过分词之后，停用词通常被过滤掉，不会被进行索引。
- 5、在检索的时候，用户的查询中如果含有停用词，检索系统也会将其过滤掉（因为用户输入的查询字符串也要进行分词处理）。
- 6、排除停用词可以加快建立索引的速度，减小索引库文件的大小。
- 7、英文停用词：http://www.ranks.nl/stopwords
- 8、中文停用词：http://www.ranks.nl/stopwords/chinese-stopwords



# settings配置
## 1 设置停用词
"common_stop": {
	"type": "stop",
	"stopwords_path": "stopwords/stopwords.dic" //自己指定对应的停用词文件目录
},

## 2 设置同义词 extension_synonym
```
"extension_synonym": {
	"type": "synonym",
	"synonyms_path": "synonyms/extension_synonym.txt",
	"lenient": "true"
}

```

- 语法：
doc => doc, docx
xls => xls, xlsx
ppt => ppt, pptx

## 3 char_filter
- https://www.elastic.co/guide/en/elasticsearch/reference/1.7/analysis-pattern-replace-charfilter.html
- 模式替换字符过滤器：模式替换字符过滤器允许在分析之前使用正则表达式操作字符串中的字符。
正则表达式是使用pattern参数定义的，替换字符串可以使用replacement参数提供（支持引用原始文本，如下所述）。
```
使用" "替换ascii值
"char_filter": {
	"ascii": {
		"pattern": "[^\\x00-\\xff]+",
		"type": "pattern_replace",
		"replacement": " "
	},
	"remove_ascii": {
		"pattern": "[\\x00-\\xff]+",
		"type": "pattern_replace",
		"replacement": " "
	}
}
```

## 4 analyzer name_ik_max_word_remove_ascii
- 一个analyzer即分析器，无论是内置的还是自定义的，只是一个包含character filters（字符过滤器）、 tokenizers（分词器）、token filters（令牌过滤器）三个细分模块的包。
- 自定义分析器（custom分析器）
- 接受以下参数：
    - type	分析器类型。接受内置分析器类型[25]。对于自定义分析器，使用custom或省略此参数。
    - tokenizer	内置或自定义tokenizer[26]。（必需的）（分词器）
        - https://jingyan.baidu.com/article/cbcede071e1b9e02f40b4d19.html 
    - 常见的几种系统内置分析器
        - 1 keyword：就是一个关键词，不进行分词操作
        - 2 ik_smart：将搜索关键词拆分成最小的几个关键词，会做最粗粒度的拆分，分词之间是不会有重复字符的，比如会将“中华人民共和国人民大会堂”拆分为中华人民共和国、人民大会堂。
        - 3 ik_max_word：会做最细细粒度拆分
        - 4 standard：标准分词器将文本划分为单词边界上的术语，这由Unicode文本分段算法定义。它删除大多数标点符号。这是大多数语言的最佳选择。
        - 5 whitespace：每当遇到任何空白字符时，空格tokenizer都会将文本划分为多个term。
        ```
        分析自定义分词器tokenizer的分词方式
        POST _analyze
        {
        	"text": "实习生 工资套",
        	"filter": [
        		"lowercase"
        	],
        	"tokenizer": "whitespace"
        }
        ```
      
        - uax_url_email：和标准分词器一样，但是把email和url当作一个词。
        UAX Email URL Tokenizer有人翻译为'不拆分email、url的分词器'，觉得不太恰当，UAX个人认为是Unicode Standard Annex，见标准分词器中。
        
        - path_hierarchy：/符号分词器
        ```
        POST _analyze 
        {
        	"text": "www/baidu/com",
        	"tokenizer":"path_hierarchy"
        }
        
        ```
        - classic：classic分词器提供基于语法的分词器，这对英语文档是一个很好的分词器。
        ```
        POST _analyze 
        {
        	"text": "you a handsame person",
        	"tokenizer":"classic"
        }
        ``` 
        
        - 自定义分析器
        - 6 ngram：ngram分词机制实现index-time搜索推荐,边际连词分词器 Edge NGram Tokenizer
          该分词器和nGram分词器很类似，但是分的词仅仅包含该词的开始部分。
          分词属性和NGram Tokenizer的一样。
          例如：中华人民共和国 全国人大 min_gram=2,max_gram=5,"token_chars":["letter","digit"]
          结果：中华、中华人、中华人民、中华人民共、全国、全国人、全国人大
          注意：单个词'中华人民共和国'的长度大于5，'和国'不会出现在分词结果中。
          **不推荐使用**：不推荐使用的NGram令牌化器中的最大值和最小值之间存在较大差异，预期差异必须小于或等于：[1]
          ```
          POST _analyze 
          {
          	"text": "实习生工资套",
          	"filter": [
          		"lowercase"
          	],
          	"tokenizer": {
          		"token_chars": [
          			"letter",
          			"digit"
          		],
          		"min_gram": "2",  最小的分词长度
          		"type": "ngram",
          		"max_gram": "2"   最大的分词长度，也就是说分出来的单词长度最大只能是2
          	}
          }
          ```
          
        - 7 split_email： 主要适用于email的分词。 @ .符号来进行分词，也可以设置更多的符号。这个自定义的token名字可以任意取。
       ``` 
        POST _analyze 
        {
        	"text": "实习生@工资.套",
        	"filter": [
        		"lowercase"
        	],
        	"tokenizer": {
        		"type": "char_group", //通过字符在进行分词
        		"tokenize_on_chars": [
        			"@",
        			"."
        		]
        	}
        }
      ```
        - 8 split_tag
        ```
        POST _analyze 
        {
        	"text": "实~习#生@工资.套",
        	"filter": [
        		"lowercase"
        	],
        	"tokenizer": {
        		"type": "char_group",
        		"tokenize_on_chars": [
        			"whitespace", 空格
        			"punctuation", 标点符号
        			"symbol"   符号
        		]
        	}
        }
        ```
        
        
    - char_filter	内置或自定义character filters[27]的可选数组 。
    - filter	内置或自定义token filters[28]的可选数组 。
    - position_increment_gap	在索引文本值数组时，Elasticsearch 在一个值的最后一项和下一个值的第一项之间插入一个假的“间隙”，以确保短语查询不匹配来自不同数组元素的两个项。默认为100. 查看position_increment_gap[29]更多。
```
"name_ik_max_word_remove_ascii": {
	"filter": [
		"lowercase",
		"name_stop"
	],
    //过滤器：删除ascii值
	"char_filter": [
		"remove_ascii"
	],
	"type": "custom",
	"tokenizer": "ik_max_word"
},

```


## 自定义分析器测试
可以使用如下方式去测试自定义的分析器的分词情况
https://www.elastic.co/guide/en/elasticsearch/reference/6.7/indices-analyze.html#indices-analyze
```GET _analyze
{
  "tokenizer" : "whitespace", //指定分词器
  "filter" : ["lowercase", {"type": "stop", "stopwords": ["a", "is", "this"]}], //过滤规则，如 小写，停用词，同义词等
  "text" : "this is a test"
}


POST _analyze
{
  "tokenizer": "keyword",
  "char_filter": ["html_strip"],
  "text":"<p>haha</p>"
}

POST _analyze
{
  "tokenizer": "standard",
  "char_filter": [
    {
      "type":"mapping",
      "mappings":["- => _"]
    }],
  "text":"010-123-1231"
}

POST _analyze
{
  "tokenizer": "standard",
  "char_filter": [
    {
      "type":"pattern_replace",
      "pattern":"http://(.*)",
      "replacement":"$1"
    }
    ],
    "text":"http://www.baidu.com"
}```



## 多fileds配置
官方对多fileds的文档：fields | Elasticsearch Guide [7.15] | Elastic，其实就是对一个字段做出多种分词处理，keyword即不分词，
 所以使用term的时候，请多注意一下，需要精准匹配原文档，请带上keyword类型的字段去查询，模糊查询就用match。