# 一 大搜车工厂直销店商家端 
1设计模式
对接口编程而不是对实现编程。
优先使用对象组合而不是继承。
- 代码冗余解决方式：**装饰器模式**
通过装饰器模式来维护在相同功能上包装成不同的解决方式（抽象出相同的处理逻辑）
如：线索，需要在线索的通用处理逻辑上进行包装，这样就符合了装饰器模式的，不用修改之前的类，就可以动态的扩展功能，是继承的一个代替模式
    - 线索功能：如我们要在处理完线索的创建逻辑之后或者之前增加一些特殊的处理逻辑，比如我们要将线索发送给特定的店铺，或者为特定类型的线索增加不同的数据
    - 团购会：我们需要根据特殊的线索类型来创建特殊的 线索轨迹：【提醒】团购会来源线索，请及时跟进。
    这里我们就用户到了线索处理的扩展。
    
    - 客户创建：订单消息，风控消息，线索消息 都需要创建客户，但是创建客户前后的逻辑不同，那么就可以对创建客户进行包装
    1 先获取到客户的一些数据 2 对客户数据进行整理成我们的需要的数据表的字段 3 创建客户 4 还有可能对客户数据进行包装成一些特定的数据通知到对应的店铺的销售
  
    
- 代码不规范的解决方式 **模板模式**：如 消息处理，都是先解析消息，在处理消息，然后在对消息进行保存，返回等后续操作。
使用模板模式规范我们的代码，使得逻辑更清晰，代码可读性更强
**统⼀逻辑封装**共享

2 使用微服务的方式，将各个模块之间进行逻辑的解耦，各个服务之间我们会提供对应的API供外部的服务使用；
模块拆分基本规则，考虑粒度，⾼内聚低耦合，如功能聚合相同业务功能场景聚
合成⼀个模块，如核⼼⾮核⼼拆分为不同模块保证稳定性低耦合，
如变动拆分经常变的和不经常变的拆分为不同模块，如qps拆分，⼀个很⾼qps
很⾼的接⼝和⼀堆qps不⾼的接⼝考虑拆分。——核⼼为考虑⼀个或多个要素进
⾏相似聚合不相似拆分
拆分之后的模块简单可分为聚合服务，具体业务场景服务，基础服务等三级，按
照树形结构向下依赖，服务之间依赖从上到下形成树形依赖，不可成环，控制向
上依赖的发⽣。
好的微服务架构需要把控变更和依赖关系


3 弹个车的客户 与 订单系统的交互、风控系统、店铺系统、销售用户系统之间的消息我们通过MQ的方式进行通讯；
如果使用相同跨服务的数据的时候，可以通过Redis的方式来获取，获取的方便而且快速；相比于Mysql 的方式减少了不少的代码的工作量

4 数据迁移：
- 一期的工厂直销店是在老的弹个车APP的层面兼容的，在二期的时候我们是创建了一个新的工厂直销店APP，
需要将所有的直销客户客户数据、线索数据、轨迹数据、任务数据迁移到我们的新的数据表中。
    - （1） 中间表的方式
    - （2） 改进：同步双写的方式，相比于中间表的方式要更简单， 老谈个车的服务与新直销系统的切换更顺畅



# 二 KOF-拳皇98终极之战OL
- 1 高并发的解决方案：基于netty的高性能网络服务器，通过非阻塞的方式、基于selector轮询，Channel的全双工，ByteBuffer缓冲区，保证了IO处理的高效性。
自研线程模型，通过非阻塞机制：根据rid取模，将不同rid（玩家）的消息放到不同的消息队列中并发处理，将同一个rid（玩家）的消息放到同一个消息队列中排队处理，替代锁方案解决并发修改共享数据问题。
- 2 服务器的缓存保证了，玩家数据获取的高效性
- 3 服务器端和客户端之间通过对于数据的加密和解密的方式来保证数据的安全性； 
    - 还有对于请求数据的解析验证是否符合我们的标准（请求Protocol code 协议号是否符合我们的范围）
    - 客户端版本号验证

- 4 我们通过反射机制，将所有的handler逻辑处理类，方法，在程序启动时通过扫描定义在类上的注解的方式，然后在通过反射将所有的类 方法信息保存在对应的HashMap中
Key为类或方法的唯一消息号（前后端也是通过这个唯一的key来获取对应的HashMap的）。
每一个请求都是通过key来获取对应的类和执行方法。

- 5 JVM优化
    - jmap -histo $pid：打印每个class的实例数目,内存占用,类全名信息。 这样我们就可以发现那些类占用的多。
        - 发现是PlayerRole 玩家的离线数据缓存太多没有对大小作出限制。并且只加载我们需要的数据，而不是去加载一个玩家的所有数据。
        老玩家的数据一般在9~10M，非常大。
        - 排行榜滥用加载玩家数据问题：一次性加载1800个玩家的数据到缓存中，是内存暴增。
    - jmap -dump:format=b,file=20170307.dump 16048：下载内存dump信息，使用**jvisualvm**工具来分析
    - 参数优化：通过观看gc日志来动态调整了CMS的回收阈值
        - -XX:CMSInitiatingOccupancyFraction=80 ：CMS的回收阈值是 80
          -XX:+UseCMSInitiatingOccupancyOnly 这两个参数，根据full gc 与
        - concurrent-mode-failure：这个异常发生原因是：CMS GC 并发清除的阶段中同时业务线程将对象放入老年代
            - 主要的原因还是预留给用户线程的空间不足
            - - **这时会启动备用预案：临时启用Serial Old收集器来重新进行老年代的垃圾收集，这样STW时间会很长。。。**
            - 为了防止这种情况，可以 提高一点预留空间，将CMSInitiatingOccupancyFraction设置为70。
            
        - **promotion failed** ：一般是进行Minor GC的时候，发现Survivor空间不够，然后晋升老年代，老年代也没有足够的空间来存放我们的年轻代对象。
            - 1 造成的主要原因：老年代空间碎片太多，造成提前的fullgc。
                - -XX:UseCMSCompactAtFullCollection：默认开启。在要fullgc之前先进行内存碎片的整合 
                - -XX:CMSFullGCBeforeCompaction=5：默认是0，也就是每次fullgc都会进行内存碎片的整合。
            - 2 还有就是 Survivor空间太小了 ：增大年轻代空间，调高Eden与 Survivor比例。
            
    - jstack pid：可以打印出线程的堆栈，然后发现死锁信息 Found one Java-level deadlock。
    
    - HashMap 的resize操作多线程情况下会使链表形成一个环形结构，查找的时候会**形成死循环。**
        - 1.7是头插法：  A-》B -》Null  ，多线程resize 情况， AB线程同时resize
        线程1扩容 先将A移入新的链表  node-》A-》B
        线程2扩容 执行发现新链表中有元素了，将node-》B-》A  这时就形成了环形结构，造成死循环。
    - 死循环：
        - 1 top查看那个进程cpu标高 
        - 2 top -Hp 12862查看进程中的那个线程cpu最高。
        - 3 jstack 线程id 查看堆栈日志。
            - 如果相同的call stack出现在同一个线程上（tid）上， 我们很很大理由相信， 这段代码可能存在较多的循环或者死循环；
- 睡眠停顿大发贼牛逼，为了获取唯一时间居然使用sleep()....
- String 常量池优化：从mongoDB加载数据时，每个字符串都是new String()，所以我们使用String.intern()，返回常量池的对象。
来降低堆内存的使用。 一般使用StringBuilder
- 
- ThreadLocal 的滥用：我们使用ThreadLocal来保存一些玩家的数据，但是有时候会发生
使用的线程不是这个玩家线程去取数据对象对当前玩家的数据操作。
    - 解决的方式：我们每次使用完对应的ThreadLocal之后 ，最好将当前线程的数据remove()掉。


# 三 客户公海
- 1 回流客户的大数据筛选条件众多，也增大了hive脚本测试的复杂度
    - 回流条件：
    - 实时回流：
    （1）未及时跟进：日间线索2小时未跟进/夜间线索24小时未跟进
    （2）主动回流：销售主动战败客户
    （3）关店客户：门店关闭后，自动回流
    （4）销售离职：离职销售，客户列表中如果拥有未处理的用户，将自动回流至公海
    -----------
    - 每日定时回流
    （1）公海领取的客户，在7天内置为战败，则在第8天统一进行回流。
    （2）超时未过风控：20天内未通过风控
    （3）超时未成交：通过风控后，30天内未成交
  - 线上测试是通过灰度城市的方式
    - 1 然后先在我们设定好的虚拟城市进行逻辑测试与数据验证 
    - 2 每开放几个城市，我们会在上线之前将回流的数据放入公海之中。如果线上发生了什么问题我们可以通过预设好的开关
    实时关闭领取功能 以免造成更多的问题。等解决了之后再上线。
 

- 2 分配方式：我们根据销售的上个月的业绩来进行客户的分配，业绩越好的可以领取的公海客户越多，上限是100
先将实时队列中的客户优先分配，不足的客户我们在通过非实时（定时任务跑出的数据）队列中获取。

- 领取公海客户并发量大，我们一城市为单位 将并发数据量进行分散，也就是以城市code为分布式锁的key中，以有可能最大的访问
并发代码块的时间作为过期时间，来保证了我们的并发数据的安全性
（我们要保证同一个客户的数据不会被不同的销售所领取）

- 超级电话：保证了我们客户数据的安全性，因为之前发现有一些客户的数据被一些店铺进行恶意的获取 在售卖客户信息的行为。
所以通过超级电话，来保证我们客户的电话信息不回被销售恶意的获取到，打电话的行为只能通过我们的平台来进行。

- 结果：在未得到充分利用的线索中，有10%的客户成为了我们的成交客户。